{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question and Answer assignment\n",
    "\n",
    "## Task overview\n",
    "\n",
    "### Formalization and definitions\n",
    "_(Adapted from the assignment's description.)_\n",
    "\n",
    "In this project, we will be focusing on one of the main features of Assist AI's technology: __macro suggestions__. Macro suggestions are recommendations of answers that are similar to a currently unanswered question.\n",
    "\n",
    "Assist AI currently approaches this task as a prediction task. More specifically, they use historical data from Customer Support tickets (for instance, a dataset containing some large number of items, where each item is a pair of a question and the correct answer to that question. Let's call this dataset D, such that D = List: ( _question_,    _answer_ ).\n",
    "\n",
    "We will also assume the use of some similarity metric over the questions, that is, some quantitative indication of how similar the currently unanswered question is with respect to any previously answered questions. The rationale behind this is that, given dataset D' and input question _q_ below:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "D' = [\n",
    "    \n",
    "    <'when was America discovered?', 'America was discovered in 1492'>,\n",
    "    <'what is the color of the sky?', 'The sky is blue'>,\n",
    "    \n",
    "]\n",
    "\n",
    "q = 'what year was America discovered?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the system should be able to recognize that D' contains a pair whose first element has a high similarity with _q_ and its answer may therefore be used as the answer for _q_, too.\n",
    "\n",
    "Besides answering previously unseen questions, this type of predictive model could also be used to determine the similarity between answers, in order to collect additional candidate answers for a given question. For illustration, let's assume we expanded dataset D' and added an item as shown below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "D' = [\n",
    "    \n",
    "    <'when was America discovered?', 'America was discovered in 1492'>,\n",
    "    <'what is the color of the sky?', 'The sky is blue'>,\n",
    "    <'when was the discovery of America?', 'Europeans first set foot on America in 1492'>\n",
    " \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we would like the similarity metric to reflect the fact that, from a semantic point of view, the answer in the first item of D' is a paraphrase of the answer in the last item of D'.\n",
    "  \n",
    "In this framework, the choice of similarity metric is very likely to be a key factor in the overall performance of the model.\n",
    "\n",
    "\n",
    "### 1st task\n",
    "Given a set of templated responses (macros) and an answer chosen from a ticket at random, describe a method that determines whether this answer belongs to or is based on one of the templated responses, and if that's the case classifies it into the right macro.\n",
    "\n",
    "\n",
    "There are a few situations to consider for this problem:\n",
    "- First of all, a reply doesn't necessarily belong to any templated answer class. __True negatives__\n",
    "- In other cases, only part of the reply will contain a templated response. __By definition__ __Single questions, not mixed in our task. If they were, the similarity with respect some specific part of the answer should be higher than the similarity with respect to the whole__\n",
    "- Finally, very often a templated response will have been used but slightly modified to suit the question, the tone of the conversation or the users' data better. __Variants__ __Handled by approximation, only 2nd task__\n",
    "__TO-DO: word embeddings, normalization__\n",
    "\n",
    "\n",
    "### 2nd task\n",
    "Now that we are able to find and put into a certain class the questions that trigger a macro response, we can build our macro classifier for suggesting macros to new unanswered questions.\n",
    "\n",
    "b) Assuming you have a dataset of questions, where each one is labelled with the macro class it belongs to, describe one method you would explore to create a classifier that can label new, previously unseen questions. Also, don't forget to explain how you'd engineer the features used to learn your classifier.\n",
    "\n",
    "Note: We can assume the training set has between 1,000 and 100,000 instances, and between 10 and 1000 classes.\n",
    "__For our purposes, # of instances is irrelevant: performance is linear and the system runs in O(N)__\n",
    "__For our purposes, # of classes is relevant and our dataset contains ~130 single-element classes__. Speed is...\n",
    "\n",
    "\n",
    "\n",
    "## Approach\n",
    "\n",
    "### Dataset\n",
    "\n",
    "### Formalization\n",
    "\n",
    "#### Features\n",
    "#### 1st versus 2nd task\n",
    "\n",
    "### Results\n",
    "\n",
    "### Error analysis\n",
    "\n",
    "## Discussion and next steps\n",
    "[From 1st task] __TO-DO: word embeddings, normalization__\n",
    "### Model enhancements\n",
    "#### Question types and expected answers\n",
    "PLACE, TIME\n",
    "#### Topic labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
