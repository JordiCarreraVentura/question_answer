{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question and Answer assignment\n",
    "\n",
    "## Task overview\n",
    "\n",
    "### Formalization and definitions\n",
    "_(Adapted from the assignment's description.)_\n",
    "\n",
    "In this project, we will be focusing on one of the main features of Assist AI's technology: __macro suggestions__. Macro suggestions consist in recommending potential answers for a question. Throughout this report, the terms *__macro__* and *__template__* will be used as synonyms unless noted otherwise.\n",
    "\n",
    "Assist AI currently approaches this problem as a prediction task. More specifically, answers to questions are extracted from historical data consisting of Customer Support tickets. In its simplest implementation, this kind of dataset will contain some large number of pairings of a question and the correct answer to that question according to some human annotator. Let's call this dataset D, such that D = List: ( _question_,    _answer_ ).\n",
    "\n",
    "We also need to assume the use of some similarity metric that allows the system to judge how similar a question and an answer are, i.e., some quantitative indication of how similar the unanswered question is with respect to a previously answered question, so that the latter's answer can also be used as a reply to the former. The rationale behind this is the following: given dataset D and input question _q_ below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D_prime = [\n",
    "    ('when was America discovered?', 'America was discovered in 1492'),\n",
    "    ('what is the color of the sky?', 'The sky is blue'),\n",
    "]\n",
    "\n",
    "q = 'what year was America discovered?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the system should be able to recognize that D' contains a pair whose first element has a high similarity with _q_ and its answer may therefore be used as the answer for _q_, too.\n",
    "\n",
    "Besides answering previously unseen questions with information from observed similar questions, this type of predictive model can also be used to determine the similarity between a new answer from the ticket history and previous answers, in order to assess e.g. if we are in front of a new answer that requires building a new template. For illustration, let's assume we expanded dataset D' and added an item as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_prime = [\n",
    "    ('when was America discovered?', 'America was discovered in 1492'),\n",
    "    ('what is the color of the sky?', 'The sky is blue')\n",
    "]\n",
    "\n",
    "#    Below is a hypothetical new ticket generated by a customer support representative.\n",
    "#    The new ticket provides a new potential <quesiton, answer> template to be learned\n",
    "#    by the system:\n",
    "new_ticket = (                                     \n",
    "    'when was the discovery of America?',          # new candidate question\n",
    "    'Europeans first set foot on America in 1492'  # new candidate answer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we would like the similarity metric to reflect the fact that, from a semantic point of view, the answer in the first item of D' is a paraphrase of the question in the new ticket and, therefore, that no new template should be added for the question 'when was the discovery of America?'.\n",
    "\n",
    "In this type of framework, the choice of similarity metric is very likely to be a key factor in the overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1st task\n",
    "On the basis of the problem description in the previous point, we will first address this second issue, namely, how to determine whether an input answer is similar to any of the answers in the templates already known by them system and, if so, which of those templates is the best match for the new question.\n",
    "\n",
    "### 2nd task\n",
    "In the second task, we will address the issue of assigning a relevant answer to a new question not previously observed in the customer support tickets from which the templates were originally extracted.\n",
    "\n",
    "> Describe one method you would explore to create a classifier that can label new, previously unseen questions.\n",
    "\n",
    "For this second part of the project, the task specifications state that we can assume the training set to contain:\n",
    "1. between 1k and 100k instances, to be denoted as _|S|_.\n",
    "2. and between 10 and 1k classes, to be denoted as _|C|_.\n",
    "\n",
    "In our implementation:\n",
    "* _|S|_ is set to 174 (all the instances in our dataset of questions and answers).\n",
    "* _|C|_ is set to 174 (ibidem).\n",
    "* For |S| = 174, a complete cycle of training and testing takes between 1.17 and 0.5 seconds, depending on the features used.\n",
    "* In its current implementation, the algorithm's training performance is approximately linear and runs in O(N) time. Generalizing from the performance observed when training on the available dataset (1.17 seconds/run), for a value of |S| = 10k we can estimate a training runtime slightly over 60 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "#### The problem of thresholding\n",
    "In order to assess the similarity between a new expression and previously observed expressions, one of the main challenges is choosing the right similarity threshold below which any candidate pair of expressions are considered dissimilar and can therefore be reliably discarded as a match.\n",
    "\n",
    "In this case, the problem lies in the fact that potentially many pairs of expressions will be similar based on some of the features used by the system to measure the similarity. This means that a simple heuristic such as picking always the highest-scoring match, will usually result in all correct matches being selected but also in many incorrect matches being selected as well (since there is always a highest-scoring match, even when the actual score is not objectively high). That is, given that the similarity scale expresses relative similarity rather than absolute similarity, we need some other way for the system to be able to detect __true negatives__ with some degree of success.\n",
    "\n",
    "In section ###, we will discuss in detail the implementation of a solution for this problem.\n",
    "\n",
    "#### The problem of partial matches\n",
    "Another issue, closely related to the problem of thresholding introduced in the previous point, is the fact that the system may miss a legitimate match between two answers due to the match existing between some of the answers' subunits rather than the full answers themselves.\n",
    "\n",
    "That is, in order to measure the similarity between a pair of answers, the system needs to take into account some ratio of the number of features that they share with respect to their total number of features (by using e.g. the Euclidean distance or some similar metric). By definition, partial matches contain all the information relevant for triggering a match, plus some amount of irrelevant information that should not trigger it. In case that the features activated by the irrelevant information outweight the features activated by the relevant information, the similarity between the two may fall below the chosen threshold, resulting in a __false negative__. \n",
    "\n",
    "Details on a potential solution for this problem are given in section ###.\n",
    "\n",
    "#### The problem of linguistic variation\n",
    "Ultimately, linguistic variation is the key linguistic challenge underlying both tasks. That is, given that natural languages allow the same thought to be expressed in a number of different ways, any similarity metric aimed at measuring the semantic relatedness between two expressions must ideally provide robustness across all, or as many of their variants as possible, for optimal performance.\n",
    "\n",
    "This type of robustness is determined by the choice of features, which must be consistent and must remain the same across variants of the same idea, i.e., feature extraction must be performed in such a way that the features extracted from two variants expressing the same overall meaning, must reflect none of the linguistic variations making the original expressions different.\n",
    "\n",
    "The details of our approach for dealing with this phenomenon are discussed in section ####."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "For our experiments, we will be using a subset of Microsoft's [WikiQA corpus](https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/).\n",
    "\n",
    "More specifically, we will be using all instances corresponding to questions with at least 2 manually-assigned correct answers (a total of 174). In addition, and as part of the test data, we will also be using a variable number of instances with a single manually-assigned correct answer (between 68 and 140 based on the experimental conditions).\n",
    "\n",
    "This subset will provide us with the data we need for tasks 1 and 2:\n",
    "* Given that the dataset contains many instances with two answers for each question, we can use one of the answers to train the system, then have the algorithm predict the label for the second answer (thus fulfilling the requirement that the prediction is tested on a previously unseen input). Since we know in advance that both are valid answers for the same question, we know what is the correct prediction we expect from the system (as a matter of fact, in practice this amounts to having a total of 348 training instances available to us).\n",
    "* Given that the dataset also contains the question that triggered each answer, it also provides us with the necessary data for our second prediction task: suggesting a potential answer for a previously unseen question. Since the questions are never used for training in our implementation, we can actually use again all of them for testing. For simplicity, we will use the same training and testing data size in both tasks:\n",
    " * 174 training instances,\n",
    " * plus between 34 and 70 positive test instances as true positives (from the subset of double-answer instances),\n",
    " * plus the same number of negative test instances as true negatives (from the subset of single-answer instances)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Architecture\n",
    "Our model is built around a simple pipeline:\n",
    "1. A main method from which all other methods and classes are called, and where all the different system [parameterizations](#Parameters) are generated.\n",
    "2. The _Dataset_ class, which is responsible for sampling the data, splitting the training and testing sets, and storing the full [corpus](#Dataset) used in our experiments.\n",
    "3. The _FeatureEngine_ class, which takes care of the [feature extraction](#Features).\n",
    "4. Two wrapper functions, each with the set of instructions for running one of the __tasks__: _FirstTask_ and _SecondTask_. Both wrappers are essentially identical except for the fact that the 2nd task's wrapper creates the test dataset from instance questions (as opposed to instance answers as is the case for the 1st task's wrapper). Therefore, in what follows we will only be looking at the [classifier](#Classifier) implemented in the 1st task wrapper.\n",
    "\n",
    "In the next sections we will cover in more detail points 1, 3 and 4 above. Point 2 has already been addressed in the previous section.\n",
    "\n",
    "### Parameters\n",
    "PARAMETER_TEST_DATA = [\n",
    "     ('0.2', 0.2),\n",
    "    ('0.4', 0.4)\n",
    "]\n",
    "\n",
    "PARAMETER_RATIO_CONFIDENCE = [\n",
    "     ('0.5', 0.5),\n",
    "     ('0.8', 0.8),\n",
    "    ('0.9', 0.9)\n",
    "]\n",
    "\n",
    "### Features\n",
    "In our model, feature extraction will based on a relatively simple set of features.\n",
    "\n",
    "1) word [n-gram](https://en.wikipedia.org/wiki/N-gram)s for _n: 1 ≤ n ≤ 3_, using the conventional definition of n-grams as\n",
    " * all possible sequences of _n_ adjacent elements in the __space__,\n",
    " * where the __space__ is defined as all the words in each sentence of the corpus\n",
    " * and where each sequence starts at the _i_-th element in the __space__ and ends at the _i + (n - 1)_-th element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 1-grams: ['this', 'sentence', 'will', 'be', 'used', 'as', 'an', 'n', 'gram', 'extraction', 'example']\n",
      "\n",
      "using 2-grams: ['this', 'sentence', 'will', 'be', 'used', 'as', 'an', 'n', 'gram', 'extraction', 'example', 'this sentence', 'sentence will', 'will be', 'be used', 'used as', 'as an', 'an n', 'n gram', 'gram extraction', 'extraction example']\n",
      "\n",
      "using 1- and 2-grams: ['this', 'sentence', 'will', 'be', 'used', 'as', 'an', 'n', 'gram', 'extraction', 'example', 'this sentence', 'sentence will', 'will be', 'be used', 'used as', 'as an', 'an n', 'n gram', 'gram extraction', 'extraction example']\n"
     ]
    }
   ],
   "source": [
    "#   Example of feature extraction using n-grams:\n",
    "from FeatureEngine import FeatureEngine\n",
    "\n",
    "fe1 = FeatureEngine(\n",
    "    ngrams=[1]\n",
    ")\n",
    "\n",
    "fe2 = FeatureEngine(\n",
    "    ngrams=[1, 2]\n",
    ")\n",
    "\n",
    "fe12 = FeatureEngine(\n",
    "    ngrams=[1, 2]\n",
    ")\n",
    "\n",
    "example = 'this sentence will be used as an n-gram extraction example'\n",
    "\n",
    "print 'using 1-grams:', fe1(example)\n",
    "print\n",
    "print 'using 2-grams:', fe2(example)\n",
    "print\n",
    "print 'using 1- and 2-grams:', fe12(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "2) character n-grams for _n: 3 ≤ n ≤ 5_, under the same n-gram definition as word n-grams above, but with the __space__ now defined as all the characters in each word of every sentence in the dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 1-grams: ['thi', 'his', 'sen', 'ent', 'nte', 'ten', 'enc', 'nce', 'wil', 'ill', 'be', 'use', 'sed', 'as', 'an', 'n', 'gra', 'ram', 'ext', 'xtr', 'tra', 'rac', 'act', 'cti', 'tio', 'ion', 'exa', 'xam', 'amp', 'mpl', 'ple']\n",
      "\n",
      "using 2-grams: ['this', 'sent', 'ente', 'nten', 'tenc', 'ence', 'will', 'be', 'used', 'as', 'an', 'n', 'gram', 'extr', 'xtra', 'trac', 'ract', 'acti', 'ctio', 'tion', 'exam', 'xamp', 'ampl', 'mple']\n",
      "\n",
      "using 1- and 2-grams: ['thi', 'his', 'this', 'sen', 'ent', 'nte', 'ten', 'enc', 'nce', 'sent', 'ente', 'nten', 'tenc', 'ence', 'wil', 'ill', 'will', 'be', 'be', 'use', 'sed', 'used', 'as', 'as', 'an', 'an', 'n', 'n', 'gra', 'ram', 'gram', 'ext', 'xtr', 'tra', 'rac', 'act', 'cti', 'tio', 'ion', 'extr', 'xtra', 'trac', 'ract', 'acti', 'ctio', 'tion', 'exa', 'xam', 'amp', 'mpl', 'ple', 'exam', 'xamp', 'ampl', 'mple']\n"
     ]
    }
   ],
   "source": [
    "#   Example of feature extraction using character n-grams:\n",
    "from FeatureEngine import FeatureEngine\n",
    "\n",
    "fe1 = FeatureEngine(\n",
    "    ch_ngrams=[3]\n",
    ")\n",
    "\n",
    "fe2 = FeatureEngine(\n",
    "    ch_ngrams=[4]\n",
    ")\n",
    "\n",
    "fe12 = FeatureEngine(\n",
    "    ch_ngrams=[3, 4]\n",
    ")\n",
    "\n",
    "example = 'this sentence will be used as an n-gram extraction example'\n",
    "\n",
    "print 'using 1-grams:', fe1(example)\n",
    "print\n",
    "print 'using 2-grams:', fe2(example)\n",
    "print\n",
    "print 'using 1- and 2-grams:', fe12(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3) word s-skip-n-grams, for _n: 3 ≤ n ≤ 4_, _S = {1, 2}_, and _s: s ∈ S_, where the final features are defined as\n",
    " * n-grams of order _n_ (also as it pertains to the __space__ from which they are extracted),\n",
    " * in which any constituents in a position _p_ of the n-gram are removed for _p = 1 + s_, as long as _p < n_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 1-grams: ['this * will', 'sentence * be', 'will * used', 'be * as', 'used * an', 'as * n', 'an * gram', 'n * extraction', 'gram * example']\n",
      "\n",
      "using 2-grams: ['this * be', 'sentence * used', 'will * as', 'be * an', 'used * n', 'as * gram', 'an * extraction', 'n * example']\n",
      "\n",
      "using 1- and 2-grams: ['this * will', 'sentence * be', 'will * used', 'be * as', 'used * an', 'as * n', 'an * gram', 'n * extraction', 'gram * example', 'this * be', 'sentence * used', 'will * as', 'be * an', 'used * n', 'as * gram', 'an * extraction', 'n * example']\n"
     ]
    }
   ],
   "source": [
    "#   Example of feature extraction using character s-skip-n-grams:\n",
    "from FeatureEngine import FeatureEngine\n",
    "\n",
    "fe1 = FeatureEngine(\n",
    "    skip_ngrams=[3]\n",
    ")\n",
    "\n",
    "fe2 = FeatureEngine(\n",
    "    skip_ngrams=[4]\n",
    ")\n",
    "\n",
    "fe12 = FeatureEngine(\n",
    "    skip_ngrams=[3, 4]\n",
    ")\n",
    "\n",
    "example = 'this sentence will be used as an n-gram extraction example'\n",
    "\n",
    "print 'using 1-grams:', fe1(example)\n",
    "print\n",
    "print 'using 2-grams:', fe2(example)\n",
    "print\n",
    "print 'using 1- and 2-grams:', fe12(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have run experiments both using different combinations of these features as well as settings where they were each disabled, in order to better assess their specific contributions. In the Results section we provide a detailed summary of the metrics.\n",
    "\n",
    "\n",
    "### Classifier\n",
    "\n",
    "#### Relevance filter _versus_ Logistic Regression\n",
    "For the initial experiments of the first task, a [Logistic Regression](https://en.wikipedia.org/wiki/Inverted_index) (LR) classifier was implemented (using the same features we described in previous sectins). However, the performance of this model was rather poor overall. After performing the error analysis, some trivial issues became apparent that could be fixed _post hoc_ with an inverse probability relevance filter, i.e., a process that only accepted a bag-of-features-based match between two items if the features in the intersection of the items' bags also happen to be the highest relevance (~highest inverse-probability) features in their respective bags.\n",
    "\n",
    "However, if that kind of filter was implemented, it would already be able to handle the classification task itself (as a mere lookup over the [inverted index](https://en.wikipedia.org/wiki/Inverted_index)), making the LR classifier redundant to a certain extent. Based on grounds both of implementation ease and theoretical elegance, a decision was finally made not to use the classifier and rely solely on the relevance filter for the time being (where the relevance filter can be seen as an inverted index mapping features to answers containing those features, and their inverse probabilities as a [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf)-like feature weighting).\n",
    "\n",
    "#### Relevance filtering as the solution for the problem of _true negatives_\n",
    "- First of all, a reply doesn't necessarily belong to any templated answer class. __True negatives__\n",
    "- In other cases, only part of the reply will contain a templated response. __By definition__ __Single questions, not mixed in my task. If they were, the similarity with respect some specific part of the answer should be higher than the similarity with respect to the whole__\n",
    "\n",
    "\n",
    "#### Solution to the partial matches problem\n",
    "\n",
    "#### Solution to the problem of linguistic variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### Metrics\n",
    "### Results\n",
    "### 1st versus 2nd task\n",
    "average of 40 experiments\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Discussion and next steps\n",
    "[From 1st task] __TO-DO: word embeddings, normalization__\n",
    "[From 1st task] __TO-DO: word embeddings, concept expansion__\n",
    "### Model enhancements\n",
    "#### Question types and expected answers\n",
    "PLACE, TIME\n",
    "__Topic labels__ already handled by the system\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
